{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8zO2mR3KhNHN07qcAvaBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/IntegrativePracticeInDataScience/blob/main/Class04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Science at Scale**\n",
        "Data Science at Scale involves analyzing large datasets to uncover patterns and insights for various applications, including process optimization and trend discovery. Key challenges include the need for robust infrastructures like cluster systems for parallel processing and vector architectures optimized for massive data through vector and matrix operations. Big Data, characterized by large, complex datasets, contrasts with Fast Data, which focuses on real-time data analysis requiring rapid processing capabilities. Effective tools like Hadoop and Spark are essential for handling Big Data, while Fast Data applications encompass areas like fraud detection and social media analysis. The scalability of cluster systems is crucial for large-scale analysis, addressing issues like data quality and interpretability. Techniques such as machine learning, data mining, and visualization are important for extracting insights, making proficiency in these areas vital for professionals in the field."
      ],
      "metadata": {
        "id": "_fmIh8k5dDif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, avg, stddev"
      ],
      "metadata": {
        "id": "hLVo1h9TdDfp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example: Scaling with Spark**\n",
        "In the following example, we start Spark and loading a large dataset. Then we perform a basic aggregation at scale to detect 'slow' requests as anomalies. Finally, we save the results."
      ],
      "metadata": {
        "id": "xuP2mmXtdN3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting Spark:\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('LargeScaleLogAnalysis') \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "IRVBn5vUdQIX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a large dataset\n",
        "import requests\n",
        "import os\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/sidsriv/Introduction-to-Data-Science-in-python/refs/heads/master/log.csv'\n",
        "local_file_path = 'log.csv' # Define a local path to save the file\n",
        "\n",
        "# Download the file\n",
        "print(f\"Downloading file from {url} to {local_file_path}...\")\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "with open(local_file_path, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# This could be a huge file in HDFS, S3, etc:\n",
        "# Now read the local file with Spark\n",
        "df = spark.read.csv(local_file_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "nNkZ0g_bdqRl",
        "outputId": "8ea8dc9c-2f48-4b70-9673-61f48d3330bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file from https://raw.githubusercontent.com/sidsriv/Introduction-to-Data-Science-in-python/refs/heads/master/log.csv to log.csv...\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting how many requests each user made:\n",
        "requests_per_user = (\n",
        "    df.groupBy('user')\n",
        "      .agg(count('*').alias('num_requests'))\n",
        ")\n",
        "\n",
        "requests_per_user.show(10)"
      ],
      "metadata": {
        "id": "2T9abL5Xd3tg",
        "outputId": "1348366e-b221-41b0-99e6-9344300a1c36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|  user|num_requests|\n",
            "+------+------------+\n",
            "|cheryl|          11|\n",
            "|   sue|          11|\n",
            "|   bob|          11|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Fast Dataâ€“style analytics:\n",
        "stats = df.select(\n",
        "    avg(col('time')).alias('avg_rt'),\n",
        "    stddev(col('time')).alias('std_rt')\n",
        ").collect()[0]\n",
        "\n",
        "threshold = stats['avg_rt'] + 3 * stats['std_rt']\n",
        "\n",
        "anomalies = df.filter(col('time') > threshold)\n",
        "\n",
        "print('Anomalies detected:')\n",
        "anomalies.show(10)"
      ],
      "metadata": {
        "id": "VnZIh16Cd_Lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc61023-e5b4-4186-d252-7dd2d8053125"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomalies detected:\n",
            "+----+----+-----+-----------------+------+------+\n",
            "|time|user|video|playback position|paused|volume|\n",
            "+----+----+-----+-----------------+------+------+\n",
            "+----+----+-----+-----------------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "requests_per_user.write.mode('overwrite').csv('output/requests_per_user')\n",
        "anomalies.write.mode('overwrite').csv('output/anomalies')\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "psWJQlDDeH7-"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}