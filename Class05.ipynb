{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCia5XGFNDx7OIGlmOGCLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/IntegrativePracticeInDataScience/blob/main/Class05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Big Data Analytics**\n",
        "The growing volume of data known as Big Data necessitates specific tools for analysis, defined by the 5Vs: Volume, Variety, Velocity, Veracity, and Value. Key frameworks include the Spark Framework, favored for real-time processing, and the Hadoop Framework, preferred for historical data analysis. Big Data analysis involves data collection, preparation, analysis, visualization, and storage, ultimately revealing patterns that drive decision-making. Forecasting in this domain leverages statistical techniques and machine learning for accurate predictions using historical data. The hybrid data analysis methodology integrates structured and unstructured data, enhancing insights through techniques like text mining. Mastery of these tools and methodologies is essential for professionals seeking to harness data's transformative potential."
      ],
      "metadata": {
        "id": "_fmIh8k5dDif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, VectorAssembler\n",
        "from pyspark.sql.functions import count\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql import Row\n",
        "import urllib.request\n",
        "import os"
      ],
      "metadata": {
        "id": "4A_AUDJuKbQh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example: Sales Forecasting Using Machine Learning**"
      ],
      "metadata": {
        "id": "EXQJ_Qa_Kf-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Spark session:\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('BigDataHybridExample') \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "clzkBSDlKhAX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97e1568a",
        "outputId": "5136d7b4-94d2-4a07-ee74-4e2f24355e09"
      },
      "source": [
        "dummy_structured_data = [\n",
        "    Row(id=1, feature_a=10, feature_b=100.5, category='A'),\n",
        "    Row(id=2, feature_a=15, feature_b=101.2, category='B'),\n",
        "    Row(id=3, feature_a=12, feature_b=99.8, category='A'),\n",
        "    Row(id=4, feature_a=20, feature_b=105.1, category='C'),\n",
        "    Row(id=5, feature_a=8, feature_b=98.0, category='B')\n",
        "]\n",
        "\n",
        "dummy_structured_df = spark.createDataFrame(dummy_structured_data)\n",
        "\n",
        "# Loading Structured Big Data (using the dummy DataFrame):\n",
        "structured_df = dummy_structured_df\n",
        "structured_df.show(5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+--------+\n",
            "| id|feature_a|feature_b|category|\n",
            "+---+---------+---------+--------+\n",
            "|  1|       10|    100.5|       A|\n",
            "|  2|       15|    101.2|       B|\n",
            "|  3|       12|     99.8|       A|\n",
            "|  4|       20|    105.1|       C|\n",
            "|  5|        8|     98.0|       B|\n",
            "+---+---------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Unstructured Big Data:\n",
        "unstructured_url = 'https://raw.githubusercontent.com/apache/spark/master/README.md' # Example raw text file\n",
        "\n",
        "local_unstructured_path = '/tmp/unstructured_data.txt' # Temporary local path\n",
        "\n",
        "# Downloading the file locally first:\n",
        "urllib.request.urlretrieve(unstructured_url, local_unstructured_path)\n",
        "\n",
        "# Reading the local file with Spark, and naming it 'reviews_df' as expected by subsequent cells:\n",
        "reviews_df = spark.read.text(local_unstructured_path)\n",
        "reviews_df.show(5)\n",
        "\n",
        "# Cleaning up the local file after use if desired:\n",
        "# os.remove(local_unstructured_path)"
      ],
      "metadata": {
        "id": "-wP_lMFKK2VV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7976d4-526a-41f7-b5ee-dab65cd6bdb9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|      # Apache Spark|\n",
            "|                    |\n",
            "|Spark is a unifie...|\n",
            "|high-level APIs i...|\n",
            "|supports general ...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning and Prepare the Text Data (Text Mining):\n",
        "tokenizer = Tokenizer(inputCol='value', outputCol='words')\n",
        "words_df = tokenizer.transform(reviews_df)\n",
        "\n",
        "remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
        "filtered_df = remover.transform(words_df)\n",
        "\n",
        "vectorizer = CountVectorizer(inputCol='filtered', outputCol='features')\n",
        "text_model = vectorizer.fit(filtered_df)\n",
        "vectorized_df = text_model.transform(filtered_df)"
      ],
      "metadata": {
        "id": "h7LyGn3pK8OR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating Text Features (Convert to Structured Form):\n",
        "# Example: Counting number of reviews (acting as proxy feature):\n",
        "text_feature_df = filtered_df.agg(count('*').alias('review_count'))\n",
        "text_feature_df.show()"
      ],
      "metadata": {
        "id": "Vwb0c5kwLMpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ece48ec-4fe5-4026-8c7b-a43af0ed970a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|review_count|\n",
            "+------------+\n",
            "|         180|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining Structured + Unstructured Data (Hybrid Analysis):\n",
        "# Adding review count to each row of sales_df for demonstration:\n",
        "\n",
        "# Create a dummy sales_df for demonstration\n",
        "from pyspark.sql import Row\n",
        "\n",
        "sales_data = [\n",
        "    Row(units_sold=100, price=10.5),\n",
        "    Row(units_sold=120, price=11.0),\n",
        "    Row(units_sold=90, price=9.8),\n",
        "    Row(units_sold=150, price=12.0),\n",
        "    Row(units_sold=80, price=9.5)\n",
        "]\n",
        "sales_df = spark.createDataFrame(sales_data)\n",
        "\n",
        "combined_df = sales_df.crossJoin(text_feature_df)\n",
        "combined_df.show(5)"
      ],
      "metadata": {
        "id": "lgsOZpEPLjWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e9446b-3374-42c5-95fb-24d5f06a35ff"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------------+\n",
            "|units_sold|price|review_count|\n",
            "+----------+-----+------------+\n",
            "|       100| 10.5|         180|\n",
            "|       120| 11.0|         180|\n",
            "|        90|  9.8|         180|\n",
            "|       150| 12.0|         180|\n",
            "|        80|  9.5|         180|\n",
            "+----------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Forecasting Using Machine Learning:\n",
        "# Using a basic regression model to predict future sales from historical features:\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['units_sold', 'price', 'review_count'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "training_data = assembler.transform(combined_df)\n",
        "\n",
        "lr = LinearRegression(featuresCol='features', labelCol='units_sold')\n",
        "model = lr.fit(training_data)\n",
        "\n",
        "print('Coefficients:', model.coefficients)\n",
        "print('Intercept:', model.intercept)"
      ],
      "metadata": {
        "id": "DWSmqerRLto8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adafd387-cd66-4a4f-edf7-e1dd9ec9413d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [1.0000000003467537,-9.870161948491333e-09,0.0]\n",
            "Intercept: 6.67795025895803e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting Future Sales:\n",
        "predictions = model.transform(training_data)\n",
        "predictions.select('units_sold', 'prediction').show(10)"
      ],
      "metadata": {
        "id": "iKTGwp3YL9yF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3e3310-18d0-4dd6-a397-bd8c0e1ae092"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|units_sold|        prediction|\n",
            "+----------+------------------+\n",
            "|       100| 99.99999999781818|\n",
            "|       120|119.99999999981817|\n",
            "|        90| 90.00000000125975|\n",
            "|       150|150.00000000035064|\n",
            "|        80| 80.00000000075326|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}